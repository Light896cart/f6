"""
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞:

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
–í—ã–∑—ã–≤–∞–µ—Ç train_stacking_with_holdout.
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–∞ hold-out –∏ –Ω–∞ –≤–Ω–µ—à–Ω–µ–º —Ç–µ—Å—Ç–µ.
–°—Ç–∞—Ç—É—Å: –û—Å–Ω–æ–≤–Ω–æ–π entry point.

"""
import numpy as np
import pandas as pd
from pathlib import Path
from src.features import preprocess_features
from src.metrics import evaluate_model
from model.stacking import train_stacking_with_holdout


def main():
    # --- –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º ---
    train_path = Path(r"train_dataset_6.csv")
    test_path = Path("test_dataset_6.csv")  # ‚Üê –¥–æ–±–∞–≤–∏–ª–∏ –ø—É—Ç—å –∫ —Ç–µ—Å—Ç—É

    if not train_path.exists():
        train_path = Path("data") / "android_device_info.csv"
        # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –µ—Å–ª–∏ –Ω–µ—Ç train/test, —Ç–æ –∏ test –Ω–µ—Ç ‚Üí –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–ª—å–∑—è
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω train_dataset.csv –∏ –Ω–µ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")

    # --- –§–∞–π–ª —Å –ø–∞–∫–µ—Ç–∞–º–∏ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω) ---
    packages_path = Path("android_packages.csv.gz")
    if not packages_path.exists():
        packages_path = Path("data") / "android_packages.csv.gz"

    # === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ TRAIN ===
    df_train = pd.read_csv(train_path)
    # df_train = preprocess_features(df_train, packages_path=str(packages_path))

    X_train = df_train.drop(columns=["target"])
    y_train = df_train["target"]

    # One-hot encoding (—Å–æ—Ö—Ä–∞–Ω–∏–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∞!)
    X_train = pd.get_dummies(X_train, dtype=int)
    train_columns = X_train.columns.tolist()  # ‚Üê –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–æ—Ä—è–¥–æ–∫ –∏ —Å–æ—Å—Ç–∞–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    # === 2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å—Ç–µ–∫–∏–Ω–≥–∞ —Å hold-out –æ—Ü–µ–Ω–∫–æ–π...")
    meta_model, base_models, X_holdout, y_holdout, holdout_pred_proba = train_stacking_with_holdout(X_train, y_train)
    evaluate_model(y_holdout, holdout_pred_proba, "‚úÖ HOLD-OUT Validation")

    # === 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ TEST ===
    if not test_path.exists():
        print("‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª test_dataset.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞.")
        return

    df_test = pd.read_csv(test_path)
    # df_test = preprocess_features(df_test, packages_path=str(packages_path))

    X_test = df_test.drop(columns=["target"])
    y_test = df_test["target"]

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ—Ç –∂–µ one-hot encoding: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–æ train
    X_test = pd.get_dummies(X_test, dtype=int)

    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –≤ —Ç–µ—Å—Ç–µ –µ—Å—Ç—å –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ —Ç—Ä–µ–π–Ω–∞ (–∏ —Ç–æ–ª—å–∫–æ –æ–Ω–∏)
    for col in train_columns:
        if col not in X_test.columns:
            X_test[col] = 0  # –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ–∏—á–∏ –∫–∞–∫ –Ω—É–ª–∏

    X_test = X_test[train_columns]  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–∞–∫ –≤ —Ç—Ä–µ–π–Ω–µ

    # === 4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ (—Ç–æ—á–Ω–æ –∫–∞–∫ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ hold-out –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π) ===
    print("üîÆ –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")

    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –¢–û–ß–ù–û –¢–ê–ö –ñ–ï, –∫–∞–∫ –≤ train_stacking_with_holdout
    lgbm_test_pred = base_models['lgbm'].predict(X_test)  # –∫–ª–∞—Å—Å—ã (0/1)
    xgb_test_pred = base_models['xgboost'].predict_proba(X_test)[:, 1]  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

    X_meta_test = np.column_stack([lgbm_test_pred, xgb_test_pred])
    test_pred_proba = meta_model.predict_proba(X_meta_test)[:, 1]

    # === 5. –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ ===
    evaluate_model(y_test, test_pred_proba, "üéØ FINAL TEST SET Evaluation")


if __name__ == "__main__":
    main()