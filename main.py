"""
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤:

- mode="preprocessed": –∏—Å–ø–æ–ª—å–∑—É–µ—Ç train_dataset_6.csv –∏ test_dataset_6.csv –Ω–∞–ø—Ä—è–º—É—é.
- mode="raw": –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –°–´–†–û–ô train (android_device_info.csv) ‚Üí train_dataset_6.csv,
              –Ω–æ —Ç–µ—Å—Ç –ë–ï–†–Å–¢–°–Ø –ì–û–¢–û–í–´–ô –∏–∑ test_dataset_6.csv (–Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∑–∞–Ω–æ–≤–æ!).
- mode="auto": –∫–∞–∫ preprocessed, –µ—Å–ª–∏ –æ–±–∞ —Ñ–∞–π–ª–∞ –µ—Å—Ç—å; –∏–Ω–∞—á–µ ‚Äî raw (–Ω–æ —Ç–µ—Å—Ç –≤—Å—ë —Ä–∞–≤–Ω–æ –≥–æ—Ç–æ–≤—ã–π).

‚ùó –í–ê–ñ–ù–û: test_dataset_6.csv —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º/—Ç–µ—Å—Ç–æ–≤—ã–º –Ω–∞–±–æ—Ä–æ–º –∏ –ù–ï –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç—Å—è.
"""

import numpy as np
import pandas as pd
from pathlib import Path
import argparse
import joblib
from src.features import preprocess_features
from src.metrics import evaluate_model
from model.stacking import train_stacking_with_holdout


def main(mode: str = "auto", save_model: bool = True):
    # --- –ü—É—Ç–∏ ---
    processed_train = Path("train_dataset_6.csv")
    processed_test = Path("test_dataset_6.csv")
    raw_train_path = Path("android_device_info.csv")
    packages_path = Path("android_packages.csv.gz")
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)

    # === –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Å—Ç ===
    if not processed_test.exists():
        raise FileNotFoundError(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª {processed_test} –û–ë–Ø–ó–ê–¢–ï–õ–ï–ù!")
    df_test = pd.read_csv(processed_test)
    if "target" not in df_test.columns:
        raise ValueError(f"{processed_test} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'target'.")

    # === –†–∞–±–æ—Ç–∞ —Å —Ç—Ä–µ–π–Ω–æ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ ===
    if mode == "preprocessed":
        print("üü¢ –†–µ–∂–∏–º: –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π train_dataset_6.csv")
        if not processed_train.exists():
            raise FileNotFoundError(f"–¢—Ä–µ–±—É–µ—Ç—Å—è {processed_train} –≤ —Ä–µ–∂–∏–º–µ 'preprocessed'")
        df_train = pd.read_csv(processed_train)


    elif mode == "raw":
        print("üü° –†–µ–∂–∏–º: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –°–´–†–û–ô —Ç—Ä–µ–π–Ω (android_device_info.csv) ‚Üí train_dataset_6.csv")
        if not raw_train_path.exists():
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—ã—Ä–æ–π —Ç—Ä–µ–π–Ω: {raw_train_path}")
        if not packages_path.exists():
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –ø–∞–∫–µ—Ç–æ–≤: {packages_path}")
        df_raw_train = pd.read_csv(raw_train_path)
        if "target" not in df_raw_train.columns or "agent_id" not in df_raw_train.columns:
            raise ValueError("–°—ã—Ä–æ–π —Ç—Ä–µ–π–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'target' –∏ 'agent_id'.")
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û —Ç—Ä–µ–π–Ω
        df_train = preprocess_features(df_raw_train, packages_path=str(packages_path))

    elif mode == "auto":
        if processed_train.exists():
            print("üü¢ –ê–≤—Ç–æ-—Ä–µ–∂–∏–º: –≥–æ—Ç–æ–≤—ã–π —Ç—Ä–µ–π–Ω –Ω–∞–π–¥–µ–Ω ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ.")
            df_train = pd.read_csv(processed_train)
            print("üü¢ –ê–≤—Ç–æ: –≥–æ—Ç–æ–≤—ã–π —Ç—Ä–µ–π–Ω –Ω–∞–π–¥–µ–Ω")
        else:
            return main(mode="raw", save_model=save_model)
    else:
        raise ValueError("mode –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'auto', 'preprocessed' –∏–ª–∏ 'raw'")

    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —Ç—Ä–µ–π–Ω–µ ===
    if "target" not in df_train.columns:
        raise ValueError("–¢—Ä–µ–π–Ω –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'target'.")

    # === –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ ===
    X_train = df_train.drop(columns=["target"])
    y_train = df_train["target"]

    X_train = pd.get_dummies(X_train, dtype=int)
    train_columns = X_train.columns.tolist()

    print("üöÄ –ó–∞–ø—É—Å–∫ —Å—Ç–µ–∫–∏–Ω–≥–∞ —Å hold-out –æ—Ü–µ–Ω–∫–æ–π...")
    meta_model, base_models, X_holdout, y_holdout, holdout_pred_proba = train_stacking_with_holdout(X_train, y_train)
    evaluate_model(y_holdout, holdout_pred_proba, "‚úÖ HOLD-OUT Validation")

    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–ª–æ–Ω–æ–∫ ===
    if save_model:
        joblib.dump(base_models, model_dir / "base_models.joblib")
        joblib.dump(meta_model, model_dir / "meta_model.joblib")
        joblib.dump(train_columns, model_dir / "train_columns.joblib")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_dir}/")

    # === –û—Ü–µ–Ω–∫–∞ –Ω–∞ –§–ò–ö–°–ò–†–û–í–ê–ù–ù–û–ú —Ç–µ—Å—Ç–µ ===
    X_test = df_test.drop(columns=["target"])
    y_test = df_test["target"]
    X_test = pd.get_dummies(X_test, dtype=int)

    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Ç—Ä–µ–π–Ω—É
    for col in train_columns:
        if col not in X_test.columns:
            X_test[col] = 0
    X_test = X_test[train_columns]

    print("üîÆ –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ (test_dataset_6.csv)...")
    lgbm_test_pred = base_models['lgbm'].predict(X_test)
    xgb_test_pred = base_models['xgboost'].predict_proba(X_test)[:, 1]
    X_meta_test = np.column_stack([lgbm_test_pred, xgb_test_pred])
    test_pred_proba = meta_model.predict_proba(X_meta_test)[:, 1]

    evaluate_model(y_test, test_pred_proba, "üéØ FINAL TEST SET Evaluation (test_dataset_6.csv)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ML-–ø–∞–π–ø–ª–∞–π–Ω —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ—Å—Ç–æ–º")
    parser.add_argument(
        "--mode",
        type=str,
        choices=["auto", "preprocessed", "raw"],
        default="preprocessed",
        help="–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: auto (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), preprocessed (–≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ), raw (–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—ã—Ä–æ–π —Ç—Ä–µ–π–Ω)"
    )
    args = parser.parse_args()

    main(mode=args.mode)