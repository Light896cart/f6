"""
–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: –æ—Ç ROC-AUC –¥–æ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ (—Å—Ç–æ–∏–º–æ—Å—Ç—å FP/FN, —ç–∫–æ–Ω–æ–º–∏—è vs baseline).
–°—Ç–∞—Ç—É—Å: –ê–∫—Ç—É–∞–ª—å–Ω—ã–π ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ main.py.
"""
import numpy as np
from sklearn.metrics import (
    classification_report, roc_auc_score, average_precision_score,
    accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix,
    matthews_corrcoef, brier_score_loss
)


def geometric_mean_score(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    return np.sqrt(sensitivity * specificity)


def evaluate_model(y_true, y_pred_proba, model_name="Model",
                   cost_fp=1200, cost_fn=5000, print_business_metrics=True):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –º–µ—Ç—Ä–∏–∫ –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –±–∏–∑–Ω–µ—Å-—É–±—ã—Ç–∫–∏.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        y_true (array): –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (0 ‚Äî Genuine, 1 ‚Äî Fraud)
        y_pred_proba (array): –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ Fraud
        model_name (str): –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞
        cost_fp (float): —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è (FP), –≤ —Ä—É–±–ª—è—Ö
        cost_fn (float): —Å—Ç–æ–∏–º–æ—Å—Ç—å –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è (FN), –≤ —Ä—É–±–ª—è—Ö
        print_business_metrics (bool): –≤—ã–≤–æ–¥–∏—Ç—å –ª–∏ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 0.3, –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ (–º–æ–∂–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ)
    y_pred = (y_pred_proba >= 0.4).astype(int)

    metrics = {
        "roc_auc": roc_auc_score(y_true, y_pred_proba),
        "pr_auc": average_precision_score(y_true, y_pred_proba),
        "accuracy": accuracy_score(y_true, y_pred),
        "balanced_accuracy": balanced_accuracy_score(y_true, y_pred),
        "f1_macro": f1_score(y_true, y_pred, average='macro'),
        "gmean": geometric_mean_score(y_true, y_pred),
        "mcc": matthews_corrcoef(y_true, y_pred),
        "brier_score": brier_score_loss(y_true, y_pred_proba)
    }

    print(f"\nüìä === {model_name} ===")
    for k, v in metrics.items():
        print(f"{k.replace('_', ' ').title():<20}: {v:.4f}")

    print("\nüìã Classification Report:")
    print(classification_report(y_true, y_pred, target_names=["Genuine", "Fraud"]))

    cm = confusion_matrix(y_true, y_pred)
    print("\nüßÆ Confusion Matrix:")
    print(cm)
    tn, fp, fn, tp = cm.ravel()
    print(f"  TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}")

    # === –ë–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∞ ===
    if print_business_metrics:
        total_business_loss = fp * cost_fp + fn * cost_fn
        n_samples = len(y_true)
        avg_loss_per_sample = total_business_loss / n_samples

        print(f"\nüíº === Business Impact (Russia, 2025 Hypotheses) ===")
        print(f"Cost per FP (block legit user) : {cost_fp:,} ‚ÇΩ")
        print(f"Cost per FN (miss fraudster)    : {cost_fn:,} ‚ÇΩ")
        print(f"Total FP                        : {fp}")
        print(f"Total FN                        : {fn}")
        print(f"üí∞ Total Expected Business Loss : {total_business_loss:,.0f} ‚ÇΩ")
        print(f"üìâ Avg Loss per Prediction      : {avg_loss_per_sample:.2f} ‚ÇΩ")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Å–∫–æ–ª—å–∫–æ "—Å–ø–∞—Å–µ–Ω–æ" –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline?
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –±—ã –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–ø—É—Å–∫–∞–ª–∏ –≤—Å–µ—Ö ‚Üí FN = total_fraud, FP = 0
        total_fraud = fn + tp
        loss_no_model = total_fraud * cost_fn  # –≤—Å–µ –º–æ—à–µ–Ω–Ω–∏–∫–∏ –ø—Ä–æ—à–ª–∏
        savings = loss_no_model - total_business_loss
        avg_savings_per_sample = savings / n_samples

        print(f"\nüí° Estimated Savings vs No Model: {savings:,.0f} ‚ÇΩ")
        print(f"üìà Avg Savings per Sample       : {avg_savings_per_sample:.2f} ‚ÇΩ")

        if savings > 0:
            print("‚úÖ Model provides positive business value.")
        else:
            print("‚ö†Ô∏è  Model may be too aggressive or ineffective ‚Äî consider tuning threshold.")

    return metrics