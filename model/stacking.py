# model/stacking.py
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.linear_model import LogisticRegression
from src.models import get_lgb_model, get_xgb_model


def train_stacking_with_holdout(X, y, n_splits=5, random_state=42):
    """
    –û–±—É—á–∞–µ—Ç —Å—Ç–µ–∫–∏–Ω–≥ —Å —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –Ω–∞ hold-out.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: meta_model, base_models, X_test, y_test, test_pred_proba
    """
    # üîë 1. Hold-out split: –æ—Ç–¥–µ–ª—è–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π —Ç–µ—Å—Ç (20%)
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X, y, test_size=0.01, stratify=y, random_state=random_state
    )

    # üîë 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è OOF-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ X_train_full —Å –ø–æ–º–æ—â—å—é StratifiedKFold
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    oof_lgbm = np.zeros(len(X_train_full))
    oof_xgb = np.zeros(len(X_train_full))

    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full)):
        X_tr, X_val = X_train_full.iloc[tr_idx], X_train_full.iloc[val_idx]
        y_tr, y_val = y_train_full.iloc[tr_idx], y_train_full.iloc[val_idx]

        # –û–±—É—á–∞–µ–º LGBM (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Booster ‚Üí .predict() –¥–∞—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
        lgb_model = get_lgb_model(X_tr, X_val, y_tr, y_val)
        oof_lgbm[val_idx] = lgb_model.predict(X_val)  # ‚Üê —É–∂–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏!

        # –û–±—É—á–∞–µ–º XGBoost (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç XGBClassifier ‚Üí .predict_proba()[:, 1])
        xgb_model = get_xgb_model(X_tr, X_val, y_tr, y_val)
        oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]

    # üîë 3. –û–±—É—á–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å (–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è) –Ω–∞ OOF-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    meta_features_train = np.column_stack([oof_lgbm, oof_xgb])
    meta_model = LogisticRegression(
        class_weight='balanced',
        random_state=random_state,
        max_iter=1000
    )
    meta_model.fit(meta_features_train, y_train_full)

    # üîë 4. –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –í–°–ï–ú X_train_full
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ø–ª–∏—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è early stopping
    X_tr_final, X_val_final, y_tr_final, y_val_final = train_test_split(
        X_train_full, y_train_full, test_size=0.01, stratify=y_train_full, random_state=random_state
    )
    final_lgb = get_lgb_model(X_tr_final, X_val_final, y_tr_final, y_val_final)
    final_xgb = get_xgb_model(X_tr_final, X_val_final, y_tr_final, y_val_final)

    # üîë 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ hold-out (X_test)
    test_meta_features = np.column_stack([
        final_lgb.predict(X_test),               # ‚Üê LGBM: .predict() ‚Üí –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        final_xgb.predict_proba(X_test)[:, 1]    # ‚Üê XGBoost: .predict_proba()[:, 1]
    ])
    test_pred_proba = meta_model.predict_proba(test_meta_features)[:, 1]

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å—ë –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ
    base_models = {'lgbm': final_lgb, 'xgboost': final_xgb}
    return meta_model, base_models, X_test, y_test, test_pred_proba